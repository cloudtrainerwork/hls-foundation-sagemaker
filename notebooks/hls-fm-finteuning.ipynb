{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fb85522-53a4-43fb-95bc-b37d953662c4",
   "metadata": {},
   "source": [
    "# HLS Foundation Model Finetuning notebook\n",
    "\n",
    "This notebook demonstrates the steps to fintune the HLS foundation model (A.K.A Prithvi) which is trained using HLSL30 and HLSS30 datasets. \n",
    "\n",
    "Note: Entierty of this notebook is desigend to work well within the AWS sagemaker environment. AWS sagemaker environment access for your account can be found using https://creds-workshop.nasa-impact.net/.\n",
    "\n",
    "![HLS Training](../images/HLS-training.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b9afa3-5424-42dc-bfcb-800df0435f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install required packages\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Create directories for data, model, and config preparations\n",
    "!mkdir -p datasets models configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200bf594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import Azure ML libraries\n",
    "from azureml.core import Workspace, Experiment, Environment, ScriptRunConfig\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.core.runconfig import MpiConfiguration\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cca8ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Initialize Azure ML Workspace\n",
    "# Method 1: From config file (if running in Azure ML compute)\n",
    "try:\n",
    "    ws = Workspace.from_config()\n",
    "    print(f\"Found workspace: {ws.name} in {ws.location}\")\n",
    "except:\n",
    "    # Method 2: Initialize manually (update with your details)\n",
    "    ws = Workspace(\n",
    "        subscription_id='your-subscription-id',\n",
    "        resource_group='your-resource-group', \n",
    "        workspace_name='your-workspace-name'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c4f1ad",
   "metadata": {},
   "source": [
    "### Download HLS Burn Scars dataset from Huggingface: https://huggingface.co/datasets/ibm-nasa-geospatial/hls_burn_scars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0c4e10-15cd-4f52-8dfe-ec04074efe43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 4: Download HLS Burn Scars dataset from Huggingface\n",
    "!cd datasets && git clone https://huggingface.co/datasets/ibm-nasa-geospatial/hls_burn_scars && tar -xvzf hls_burn_scars/hls_burn_scars.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef75557",
   "metadata": {},
   "source": [
    "## Download config and Pre-trained model\n",
    "\n",
    "The HLS Foundation Model (pre-trained model), and configuration for Burn Scars downstream task are available in Huggingface. We use `huggingface_hub` python package to download the files locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe11e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Define constants - UPDATE THESE VALUES\n",
    "STORAGE_ACCOUNT_NAME = 'your-storage-account'  # Replace with your storage account\n",
    "CONTAINER_NAME = 'hls-data'  # Replace with your container name\n",
    "CONFIG_PATH = './configs'\n",
    "DATASET_PATH = './datasets' \n",
    "MODEL_PATH = './models'\n",
    "EXPERIMENT_NAME = 'hls-foundation-training'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f74b4d",
   "metadata": {},
   "source": [
    "Note: The configuration file in Huggingface have place holders and won't be directly usable for fine-tuning. Placeholder values need to be updated for individual usecases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d6b5d-d58d-4b90-a952-6179c255280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "hf_hub_download(\n",
    "    repo_id=\"ibm-nasa-geospatial/Prithvi-100M-burn-scar\",\n",
    "    filename=\"burn_scars_Prithvi_100M.py\", \n",
    "    local_dir='./configs'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff4ddab-fd48-4015-9822-17047d3a4039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Update configuration file for Azure ML\n",
    "# Read the config file and update paths for Azure ML\n",
    "with open('./configs/burn_scars_Prithvi_100M.py', 'r') as f:\n",
    "    config_content = f.read()\n",
    "\n",
    "# Replace paths for Azure ML environment\n",
    "config_content = config_content.replace(\n",
    "    \"data_root = '<path to data root>'\",\n",
    "    \"data_root = '/tmp/data/'\"\n",
    ")\n",
    "config_content = config_content.replace(\n",
    "    \"pretrained_weights_path = '<path to pretrained weights>'\", \n",
    "    \"pretrained_weights_path = f\\\"{data_root}/models/prithvi-global-300M.pt\\\"\"\n",
    ")\n",
    "config_content = config_content.replace(\n",
    "    \"experiment = '<experiment name>'\",\n",
    "    \"experiment = 'burn_scars'\"\n",
    ")\n",
    "config_content = config_content.replace(\n",
    "    \"project_dir = '<project directory name>'\",\n",
    "    \"project_dir = 'v1'\"\n",
    ")\n",
    "\n",
    "# Save updated config\n",
    "with open('./configs/burn_scars_Prithvi_100M.py', 'w') as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "print(\"✅ Configuration file updated for Azure ML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01d77ad-728e-430c-bf59-f3cb36e19592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Upload data to Azure Blob Storage and register as datasets\n",
    "def upload_to_blob_and_register_dataset(local_path, dataset_name, description):\n",
    "    \"\"\"Upload local data to blob storage and register as Azure ML dataset\"\"\"\n",
    "    \n",
    "    # Get default datastore\n",
    "    datastore = ws.get_default_datastore()\n",
    "    \n",
    "    # Upload data\n",
    "    print(f\"Uploading {local_path} to datastore...\")\n",
    "    datastore.upload(\n",
    "        src_dir=local_path,\n",
    "        target_path=dataset_name,\n",
    "        overwrite=True,\n",
    "        show_progress=True\n",
    "    )\n",
    "    \n",
    "    # Register as dataset\n",
    "    dataset = Dataset.File.from_files(\n",
    "        path=[(datastore, dataset_name)],\n",
    "        validate=False\n",
    "    ).register(\n",
    "        workspace=ws,\n",
    "        name=dataset_name,\n",
    "        description=description,\n",
    "        create_new_version=True\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Registered dataset: {dataset_name}\")\n",
    "    return dataset\n",
    "\n",
    "# Upload training, validation, and test data\n",
    "train_dataset = upload_to_blob_and_register_dataset(\n",
    "    'datasets/training', \n",
    "    'hls-training-data',\n",
    "    'HLS burn scars training data'\n",
    ")\n",
    "\n",
    "val_dataset = upload_to_blob_and_register_dataset(\n",
    "    'datasets/validation',\n",
    "    'hls-validation-data', \n",
    "    'HLS burn scars validation data'\n",
    ")\n",
    "\n",
    "test_dataset = upload_to_blob_and_register_dataset(\n",
    "    'datasets/validation',  # Note: using validation for test as in original\n",
    "    'hls-test-data',\n",
    "    'HLS burn scars test data'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a2384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Setup identifier and upload config/model files\n",
    "identifier = input(\"Enter your identifier (e.g., your-name): \") or \"user\"\n",
    "\n",
    "# Rename config file\n",
    "config_filename = 'configs/burn_scars_Prithvi_100M.py'\n",
    "new_config_filename = f\"configs/{identifier}-burn_scars_Prithvi_100M.py\"\n",
    "os.rename(config_filename, new_config_filename)\n",
    "\n",
    "# Upload config to datastore\n",
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload_files(\n",
    "    files=[new_config_filename],\n",
    "    target_path='configs/',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Download and upload pretrained model\n",
    "print(\"Downloading pretrained model...\")\n",
    "hf_hub_download(\n",
    "    repo_id=\"ibm-nasa-geospatial/Prithvi-100M\",\n",
    "    filename=\"Prithvi_100M.pt\",\n",
    "    local_dir='./models',\n",
    "    local_dir_use_symlinks=False\n",
    ")\n",
    "\n",
    "# Upload model to datastore\n",
    "datastore.upload_files(\n",
    "    files=['./models/Prithvi_100M.pt'],\n",
    "    target_path='models/',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "print(\"✅ Config and model files uploaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18302154-eee5-4705-b1c4-11cfd47ef6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Create or get compute target\n",
    "compute_name = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=compute_name)\n",
    "    print(f\"Found existing compute target: {compute_name}\")\n",
    "except ComputeTargetException:\n",
    "    print(f\"Creating new compute target: {compute_name}\")\n",
    "    \n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"Standard_NC6s_v3\",  # Equivalent to ml.p3.2xlarge (V100 GPU)\n",
    "        min_nodes=0,\n",
    "        max_nodes=1,\n",
    "        idle_seconds_before_scaledown=300\n",
    "    )\n",
    "    \n",
    "    compute_target = ComputeTarget.create(ws, compute_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bbbf5a-6df2-44d7-a8bc-81026631d41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Create custom environment\n",
    "from azureml.core import Environment\n",
    "from azureml.core.environment import DockerBuildContext\n",
    "\n",
    "# Create environment from your Dockerfile\n",
    "env = Environment(name=\"hls-foundation-env\")\n",
    "\n",
    "# Option 1: Use your custom Docker image (if pushed to ACR)\n",
    "# env.docker.base_image = \"your-acr.azurecr.io/azureml-hls:latest\"\n",
    "# env.python.user_managed_dependencies = True\n",
    "\n",
    "# Option 2: Build from Dockerfile (recommended)\n",
    "dockerfile_path = \".\"  # Path to your Dockerfile\n",
    "env.docker.build_context = DockerBuildContext.from_local_directory(dockerfile_path)\n",
    "env.docker.dockerfile_path = \"Dockerfile\"\n",
    "\n",
    "# Register environment\n",
    "env.register(workspace=ws)\n",
    "\n",
    "print(\"✅ Environment created and registered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c98a7a-f199-49c6-a655-38c19dd28689",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 12: Setup training configuration\n",
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "# Environment variables for Azure ML (equivalent to SageMaker environment)\n",
    "environment_variables = {\n",
    "    'CONFIG_FILE': f\"/tmp/data/configs/{identifier}-burn_scars_Prithvi_100M.py\",\n",
    "    'MODEL_NAME': f\"{identifier}-workshop.pth\",\n",
    "    'AZURE_BLOB_URL': f\"https://{STORAGE_ACCOUNT_NAME}.blob.core.windows.net/{CONTAINER_NAME}\",\n",
    "    'EVENT_TYPE': 'burn_scars',\n",
    "    'VERSION': 'v1',\n",
    "    'DATASET_NAME': 'hls-training-data'  # For dataset-based loading\n",
    "}\n",
    "\n",
    "# Create script run configuration\n",
    "script_config = ScriptRunConfig(\n",
    "    source_directory='.',  # Directory containing train.py and utils.py\n",
    "    script='train.py',\n",
    "    environment=env,\n",
    "    compute_target=compute_target,\n",
    "    environment_variables=environment_variables\n",
    ")\n",
    "\n",
    "# Add dataset inputs\n",
    "script_config.run_config.data_references[train_dataset.name] = train_dataset.as_named_input('training').as_mount()\n",
    "script_config.run_config.data_references[val_dataset.name] = val_dataset.as_named_input('validation').as_mount()  \n",
    "script_config.run_config.data_references[test_dataset.name] = test_dataset.as_named_input('test').as_mount()\n",
    "\n",
    "print(\"✅ Training configuration setup complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc35c27d-eba9-4c9a-a28f-292848b982b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Create and submit experiment\n",
    "experiment = Experiment(workspace=ws, name=EXPERIMENT_NAME)\n",
    "\n",
    "print(f\"Submitting experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"Compute target: {compute_target.name}\")\n",
    "print(f\"Environment: {env.name}\")\n",
    "\n",
    "# Submit the training run\n",
    "run = experiment.submit(script_config)\n",
    "\n",
    "print(f\"✅ Training job submitted!\")\n",
    "print(f\"Run ID: {run.id}\")\n",
    "print(f\"Monitor progress at: {run.get_portal_url()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18f273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Monitor training (optional - uncomment to wait for completion)\n",
    "# run.wait_for_completion(show_output=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8987afb44532b2110e1a5e1b229dd281f8440b44477d285826a54acdd52d8797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
